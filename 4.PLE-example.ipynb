{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "4.PLE-example.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BKaDNTCPZl4n"
      },
      "source": [
        "# PLE多任务学习建模"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s6srBeg382ee"
      },
      "source": [
        "## 加载数据"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kloudYEG6Cws"
      },
      "source": [
        "import joblib\n",
        "train = joblib.load('./data_and_feature/train.txt')\n",
        "val = joblib.load('./data_and_feature/val.txt')\n",
        "test = joblib.load('./data_and_feature/test.txt')\n",
        "encoder = joblib.load('./data_and_feature/encoder.txt')\n",
        "\n",
        "train_num = len(train)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TfVav1G3NM_Q"
      },
      "source": [
        "## 导入工具库"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "atU3210yKot0"
      },
      "source": [
        "import numpy as np\n",
        "\n",
        "import gc\n",
        "import os\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "import tensorflow as tf\n",
        "import tensorflow.keras.backend as K\n",
        "\n",
        "from tensorflow.keras.layers import *\n",
        "from tensorflow.python.keras.layers import Layer\n",
        "from tensorflow.keras import regularizers\n",
        "\n",
        "from tensorflow.keras.models import Model,load_model\n",
        "from tensorflow.keras.utils import plot_model\n",
        "from tensorflow.keras.callbacks import ModelCheckpoint,ReduceLROnPlateau,EarlyStopping\n",
        "\n",
        "from tensorflow.keras import optimizers,initializers\n",
        "from tensorflow.python.keras.initializers import glorot_normal"
      ],
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XlJgKlOlNTcV"
      },
      "source": [
        "## 搭建PLE模型"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6FunCHvVMzKq"
      },
      "source": [
        "class MeanPoolLayer(Layer):\n",
        "    def __init__(self, axis, **kwargs):\n",
        "        super(MeanPoolLayer, self).__init__(**kwargs)\n",
        "        self.axis = axis\n",
        "\n",
        "    def call(self, x, mask):\n",
        "        mask = tf.expand_dims(tf.cast(mask,tf.float32),axis = -1)\n",
        "        x = x * mask\n",
        "        return K.sum(x, axis=self.axis) / (K.sum(mask, axis=self.axis) + 1e-9)\n",
        "\n",
        "class PleLayer(tf.keras.layers.Layer):\n",
        "    '''\n",
        "    n_experts:list,每个任务使用几个expert。[2,3]第一个任务使用2个expert，第二个任务使用3个expert。\n",
        "    n_expert_share:int,共享的部分设置的expert个数。\n",
        "    expert_dim:int,每个专家网络输出的向量维度。\n",
        "    n_task:int,任务个数。\n",
        "    '''\n",
        "    def __init__(self,n_task,n_experts,expert_dim,n_expert_share,dnn_reg_l2 = 1e-5):\n",
        "        super(PleLayer, self).__init__()\n",
        "        self.n_task = n_task\n",
        "        \n",
        "        # 生成多个任务特定网络和1个共享网络。\n",
        "        self.E_layer = []\n",
        "        for i in range(n_task):\n",
        "            sub_exp = [Dense(expert_dim,activation = 'relu') for j in range(n_experts[i])]\n",
        "            self.E_layer.append(sub_exp)\n",
        "            \n",
        "        self.share_layer = [Dense(expert_dim,activation = 'relu') for j in range(n_expert_share)]\n",
        "        #定义门控网络\n",
        "        self.gate_layers = [Dense(n_expert_share+n_experts[i],kernel_regularizer=regularizers.l2(dnn_reg_l2),\n",
        "                                  activation = 'softmax') for i in range(n_task)]\n",
        "\n",
        "    def call(self,x):\n",
        "        #特定网络和共享网络\n",
        "        E_net = [[expert(x) for expert in sub_expert] for sub_expert in self.E_layer]\n",
        "        share_net = [expert(x) for expert in self.share_layer]\n",
        "        \n",
        "        #门的权重乘上，指定任务和共享任务的输出。\n",
        "        towers = []\n",
        "        for i in range(self.n_task):\n",
        "            g = self.gate_layers[i](x)\n",
        "            g = tf.expand_dims(g,axis = -1) #(bs,n_expert_share+n_experts[i],1)\n",
        "            _e = share_net+E_net[i]  \n",
        "            _e = Concatenate(axis = 1)([expert[:,tf.newaxis,:] for expert in _e]) #(bs,n_expert_share+n_experts[i],expert_dim)\n",
        "            _tower = tf.matmul(_e, g,transpose_a=True)\n",
        "            towers.append(Flatten()(_tower)) #(bs,expert_dim)\n",
        "        return towers\n",
        "\n",
        "def build_ple(sparse_cols,dense_cols,sparse_max_len,embed_dim,expert_dim = 4,\n",
        "              varlens_cols = [],varlens_max_len = [],dnn_hidden_units = (64,64),\n",
        "              n_task = 2,n_experts = [2,2],n_expert_share = 4,dnn_reg_l2 = 1e-6,\n",
        "              drop_rate = 0.0,embedding_reg_l2 = 1e-6,targets = []):\n",
        "\n",
        "   #输入部分，分为sparse,varlens,dense部分。\n",
        "    sparse_inputs = {f:Input([1],name = f) for f in sparse_cols}\n",
        "    dense_inputs = {f:Input([1],name = f) for f in dense_cols}\n",
        "    varlens_inputs = {f:Input([None,1],name = f) for f in varlens_cols}\n",
        "        \n",
        "    input_embed = {}\n",
        "    #离散特征，embedding到k维\n",
        "    for f in sparse_cols:\n",
        "        _input = sparse_inputs[f]\n",
        "        embedding = Embedding(sparse_max_len[f], embed_dim, \n",
        "            embeddings_regularizer=tf.keras.regularizers.l2(embedding_reg_l2)) \n",
        "        input_embed[f] =Flatten()(embedding(_input)) #(bs,k)\n",
        "        \n",
        "    #多标签离散变量\n",
        "    for f in varlens_inputs:\n",
        "        _input = varlens_inputs[f]\n",
        "        mask = Masking(mask_value = 0).compute_mask(_input)\n",
        "        embedding = Embedding(varlens_max_len[f], embed_dim,\n",
        "            embeddings_regularizer=tf.keras.regularizers.l2(1e-6))\n",
        "        _embed =Reshape([-1,embed_dim])(embedding(_input))\n",
        "        out_embed = MeanPoolLayer(axis=1)(_embed,mask)\n",
        "        input_embed[f] = out_embed\n",
        "        \n",
        "    input_embed.update(dense_inputs) #加入连续变量\n",
        "    input_embed = Concatenate(axis = -1)([input_embed[f] for f in input_embed])    \n",
        "                                  \n",
        "    for num in dnn_hidden_units:\n",
        "        input_embed = Dropout(drop_rate)(Dense(num,activation = 'relu',\n",
        "                    kernel_regularizer=regularizers.l2(dnn_reg_l2))(input_embed))\n",
        "    #Ple网络层\n",
        "    towers = PleLayer(n_task,n_experts,expert_dim,n_expert_share)(input_embed)\n",
        "    outputs = [Dense(1,activation = 'sigmoid',kernel_regularizer=regularizers.l2(dnn_reg_l2),\n",
        "                       name = f,use_bias = True)(_t) for f,_t in zip(targets,towers)]\n",
        "    inputs = [sparse_inputs[f] for f in sparse_inputs]+[varlens_inputs[f] for f in varlens_inputs]\\\n",
        "                +[dense_inputs[f] for f in dense_inputs]\n",
        "    model = Model(inputs,outputs) \n",
        "    return model"
      ],
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hSbZkTbuOzYp",
        "outputId": "c01c962f-05e2-47d4-dac2-c667066669a0"
      },
      "source": [
        "target = [\"read_comment\", \"like\", \"click_avatar\", \"forward\"]\n",
        "sparse_features = ['userid', 'feedid', 'authorid', 'bgm_song_id', 'bgm_singer_id']\n",
        "varlen_features = ['manual_tag_list','manual_keyword_list']\n",
        "dense_features = ['videoplayseconds']\n",
        "\n",
        "# 生成输入特征设置\n",
        "sparse_max_len = {f:len(encoder[f]) + 1 for f in sparse_features}\n",
        "varlens_max_len = {f:len(encoder[f]) + 1 for f in varlen_features}\n",
        "feature_names = sparse_features+varlen_features+dense_features\n",
        "\n",
        "# 构建输入数据\n",
        "train_model_input = {name: train[name] if name not in varlen_features else np.stack(train[name]) for name in feature_names } #训练模型的输入，字典类型。名称和具体值\n",
        "val_model_input = {name: val[name] if name not in varlen_features else np.stack(val[name]) for name in feature_names }\n",
        "test_model_input = {name: test[name] if name not in varlen_features else np.stack(test[name]) for name in feature_names}\n",
        "\n",
        "train_labels = [train[y].values for y in target]\n",
        "val_labels = [val[y].values for y in target]\n",
        "\n",
        "# 删除多余的数据，释放内存\n",
        "del train,val\n",
        "gc.collect()\n",
        "\n",
        "# 构建模型，训练和评估\n",
        "model = build_ple(sparse_features,dense_features,sparse_max_len,embed_dim = 16,expert_dim = 32,\n",
        "          varlens_cols = varlen_features,varlens_max_len = varlens_max_len,dnn_hidden_units = (64,),\n",
        "          n_task = 4,n_experts = [4,4,4,4],n_expert_share = 8,dnn_reg_l2 = 1e-6,\n",
        "          drop_rate = 0.1,embedding_reg_l2 = 1e-6,targets = target)\n",
        "\n",
        "adam = optimizers.Adam(learning_rate=0.01, beta_1=0.9, beta_2=0.999, epsilon=None, decay=0.0, amsgrad=False)\n",
        "model.compile(adam, loss = 'binary_crossentropy' ,metrics = [tf.keras.metrics.AUC()],)\n",
        "\n",
        "history = model.fit(train_model_input, train_labels,validation_data = (val_model_input,val_labels),\n",
        "                    batch_size=10240, epochs=4, verbose=1)"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/4\n",
            "656/656 [==============================] - 60s 78ms/step - loss: 0.2822 - read_comment_loss: 0.1019 - like_loss: 0.0989 - click_avatar_loss: 0.0423 - forward_loss: 0.0263 - read_comment_auc: 0.9084 - like_auc: 0.8201 - click_avatar_auc: 0.7767 - forward_auc: 0.7613 - val_loss: 0.2538 - val_read_comment_loss: 0.0936 - val_like_loss: 0.0905 - val_click_avatar_loss: 0.0364 - val_forward_loss: 0.0186 - val_read_comment_auc: 0.9194 - val_like_auc: 0.8285 - val_click_avatar_auc: 0.8236 - val_forward_auc: 0.7960\n",
            "Epoch 2/4\n",
            "656/656 [==============================] - 49s 75ms/step - loss: 0.2495 - read_comment_loss: 0.0899 - like_loss: 0.0892 - click_avatar_loss: 0.0352 - forward_loss: 0.0193 - read_comment_auc: 0.9359 - like_auc: 0.8629 - click_avatar_auc: 0.8484 - forward_auc: 0.8568 - val_loss: 0.2522 - val_read_comment_loss: 0.0924 - val_like_loss: 0.0892 - val_click_avatar_loss: 0.0368 - val_forward_loss: 0.0183 - val_read_comment_auc: 0.9238 - val_like_auc: 0.8365 - val_click_avatar_auc: 0.8240 - val_forward_auc: 0.8120\n",
            "Epoch 3/4\n",
            "656/656 [==============================] - 49s 74ms/step - loss: 0.2475 - read_comment_loss: 0.0887 - like_loss: 0.0883 - click_avatar_loss: 0.0346 - forward_loss: 0.0190 - read_comment_auc: 0.9389 - like_auc: 0.8675 - click_avatar_auc: 0.8567 - forward_auc: 0.8675 - val_loss: 0.2523 - val_read_comment_loss: 0.0923 - val_like_loss: 0.0891 - val_click_avatar_loss: 0.0362 - val_forward_loss: 0.0182 - val_read_comment_auc: 0.9244 - val_like_auc: 0.8361 - val_click_avatar_auc: 0.8255 - val_forward_auc: 0.8159\n",
            "Epoch 4/4\n",
            "656/656 [==============================] - 49s 74ms/step - loss: 0.2466 - read_comment_loss: 0.0881 - like_loss: 0.0878 - click_avatar_loss: 0.0342 - forward_loss: 0.0187 - read_comment_auc: 0.9402 - like_auc: 0.8702 - click_avatar_auc: 0.8622 - forward_auc: 0.8740 - val_loss: 0.2522 - val_read_comment_loss: 0.0926 - val_like_loss: 0.0886 - val_click_avatar_loss: 0.0361 - val_forward_loss: 0.0182 - val_read_comment_auc: 0.9245 - val_like_auc: 0.8390 - val_click_avatar_auc: 0.8202 - val_forward_auc: 0.8220\n"
          ]
        }
      ]
    }
  ]
}